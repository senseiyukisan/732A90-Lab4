---
title: 'Computational Statistics: Lab 4'
author: "Nicolas Taba & Yuki Washio"
date: "30/11/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Computations with Metropolis-Hastings

### 1

```{r, echo = TRUE}

# rm(list = ls())
library(ggplot2)
# target probability density function 
f_target <- function(x){
  return((x^5)*exp(-x))
}


# Need to change the plotting because the horizontal lines are calculated from the original code (2sd away from mean in each direction)
#In principal, this works
f.MCMC.MH.lnorm<-function(nstep,X0,props){
  vN<-1:nstep
  vX<-rep(X0,nstep);
  for (i in 2:nstep){
    X<-vX[i-1]
    Y<-rlnorm(1,meanlog = X,sdlog=props)
    u<-runif(1)
    a<-min(c(1,(dlnorm(Y)*dlnorm(X,meanlog=Y,sdlog=props))/(dlnorm(X)*dlnorm(Y,meanlog=X,sdlog =props))))
    if (u <=a){vX[i]<-Y}else{vX[i]<-X}    
  }
  plot(vN,vX,pch=19,cex=0.3,col="black",xlab="t",ylab="X(t)",main="",ylim=c(min(X0-0.5,-5),max(5,X0+0.5)))
  abline(h=0)
  abline(h=1.96)
  abline(h=-1.96)
}

f.MCMC.MH.lnorm(nstep = 50000, X0 = 1, props = f_target(1))
```

There is no burn-in period observed. The time series returns stable values for several time-steps before changing values. This means that several values are rejected by the algorithm. 
<!-- (The question is asking for a burn-in period so it seems that our question is not correct) -->

### 2

```{r}
#########
### 2 ###
#########
# Doesn't work. Might be worth writing our own MH algorithm based on the slides.
# Not sure what I am supposed to floor here
f.MCMC.MH.chisq<-function(nstep,X0,props){
  vN<-1:nstep
  vX<-rep(X0,nstep);
  for (i in 2:nstep){
    X<-vX[i-1]
    Y<-rchisq(1,df = floor(X))
    u<-runif(1)
    a<-min(c(1,(dchisq(Y)*dchisq(X,df = floor(Y)))/(dchisq(X)*dchisq(Y,df=floor(X)))))
    if (u <=a){vX[i]<-Y}else{vX[i]<-X}    
  }
  plot(vN,vX,pch=19,cex=0.3,col="black",xlab="t",ylab="X(t)",main="",ylim=c(min(X0-0.5,-5),max(5,X0+0.5)))
  abline(h=0)
  abline(h=1.96)
  abline(h=-1.96)
}

f.MCMC.MH.chisq(nstep = 10000, X0 = 1, props = f_target(1))
```

### 3

<!-- Probably the second distribution works pretty well and we will see the burn in period and everything converging nicely. -->


### 4


```{r}
#########
### 4 ###
#########
library(coda) #this library has a gelman.diag()function to calculate the gelman-rubin factor immediately
#see code for slide 22

## Bullshit to have an idea of what to do
for (i in 1:10){
  do_MCMC_chisq(nstep = 10000, X0 = i, props = f(target(i)))
}

# Store itemas a mcmc.list
mcmc_item = mcmc.list()
for(i in 1:10){
  mcmc_item[[i]] = as.mcmc(do_MCMC_chisq[[i]]) #proably we need to specify the sample here
}

gelman_factor = gelman.diag(mcmc_item)

## Result is probably close to 1 and shows convergence
```

### 5


### 6

This integral represents the calculation of the expected value of a gamma distribution with parameters $\alpha = 6$ and $_beta = 1$ up to a constant. We know that the expectation value of the distribution over the whole domain is equal to its mean. The mean of a gamma distribution is $\alpha\beta$, therefore the integral can be solved exactly and is equal to 6.



## Gibbs sampling

### 1

```{r}
load("chemical.RData")
data = data.frame("X" = X,"Y" = Y)

plot1 <- ggplot(data, aes(x = X, y = Y))+
  geom_point()+
  geom_line()
plot1
```

This curve looks quadratic with a negative value to its quadratic term and a positive value for its other terms.

### 2

The probability distribution function of a gaussian distribution is:

$$N(\mu,\,\sigma^{2}) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y_i-\mu_i)^2}{2\sigma^2}}$$
We calculate the Likelihood as the product of successive probability distributions:

$$p(Y|\mu) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \cdot exp \Bigg{(} -\frac{(y_i-\mu_i)^2}{2\sigma^2} \Bigg{)} = \bigg{(}\frac{1}{\sqrt{2\pi\sigma^2}}\bigg{)}^n  \cdot exp \Bigg{(}  -\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-\mu_i)^2 \Bigg{)}$$

For the calculation of the prior, the first element of the product of probabilities is just equal to 1, we therefore compute the product of the normal distributions from 1 to n-1. The prior is thus:

$$p(\mu) = \bigg{(}\frac{1}{\sqrt{2\pi\sigma^2}}\bigg{)}^n \cdot  exp \Bigg{(} -\frac{1}{2\sigma^2}\sum_{i = 1}^{n-1}(\mu_{i+1}-\mu_i)^2 \Bigg{)}$$

### 3

The posterior is:

$$p(\vec\mu|\vec{Y}) \propto P(\vec{Y}|\vec{\mu})P(\vec{\mu})$$
We compute the product and associate terms under the exponential. We then isolate the last term of the sum of the likelihood and perform the sum from 1 to n-1:

$$p(\vec\mu|\vec{Y}) \propto \text{exp} \Bigg{[}  -\frac{1}{2\sigma^2}\sum_{i=1}^{n}({y_i}-{\mu_i})^2 \Bigg{]} \text{exp} \Bigg{[} -\frac{1}{2\sigma^2}\sum_{i = 1}^{n-1}({\mu_{i+1}}-{\mu_i})^2 \Bigg{]}$$

$$p(\vec\mu|\vec{Y}) \propto exp \Bigg{[} -\frac{1}{2\sigma^2} \bigg{(} \sum_{i=1}^{n}({y_i}-{\mu_i})^2 + \sum_{i = 1}^{n-1}({\mu_{i+1}}-{\mu_i})^2 \bigg{)} \Bigg{]} \propto exp \Bigg{[} -\frac{1}{2\sigma^2} \bigg{(} \sum^{n-1}_{i=1} \big{[} (\mu_i - \mu_{i+1})^2 + (\mu_i -  y_i)^2 \big{]} + (\mu_n - y_n)^2 \big{)} \Bigg{]}$$

From here, we can compute the posterior probability at each step. For the first and last step, the calculation is straightforward making use of hint B given in the exercise sheet. However, for middle steps, we encounter an issue in the generalization of the calculation of the prior in the middle of our sequence. the i-th prior's depends on the previous value and the one that follows.

FOr the first step, we have:

$$p(\mu_1 | \vec\mu_{-1},\vec Y) \propto exp \Bigg{[} -\frac{1}{2\sigma^2} \bigg{[}  (\mu_1- \mu_2)^2 + (\mu_1 - y_1)^2  \bigg{]} \Bigg{]}$$

Frr the last step we have:

$$p(\mu_n | \vec\mu_{-n}, \vec Y) \propto exp \Bigg{[} -\frac{1}{2\sigma^2}  \bigg{[}  (\mu_{n-1} - \mu_{n})^2 + (\mu_n - y_{n})^2  \bigg{]}  \Bigg{]} $$
We can use hint B to transform both these expressions respectively into:

$$p(\mu_1 | \vec\mu_{-1},\vec Y)  \propto exp \Bigg{[} -\frac{1}{\sigma^2} \bigg{[} \mu_1 - \frac{\mu_2+y_1}{2} \bigg{]}^2 \Bigg{]} \sim N \Bigg{(}\frac{\mu_2+y_1}{2}, \frac{\sigma^2}{2} \Bigg{)}$$
 and 
 
 $$p(\mu_n | \vec\mu_{-n},\vec Y)  \propto exp \Bigg{[} -\frac{1}{\sigma^2} \bigg{[} \mu_n - \frac{\mu_{n-1}+y_n}{2} \bigg{]}^2 \Bigg{]} \sim N \Bigg{(} \frac{\mu_{n-1}+y_n}{2}, \frac{\sigma^2}{2} \Bigg{)}$$
For the middle step we have terms that show dependency on the previous and the next step in the calculation of the prior-term:

$$p(\mu_i | \vec\mu_{-i},\vec Y) \propto exp \Bigg{[} -\frac{1}{2\sigma^2} \bigg{[}  (\mu_{i-1}- \mu_{i})^2+ (\mu_i- \mu_{i+1})^2 + (\mu_{i} - y_i)^2  \bigg{]} \Bigg{]}$$

 We now use hint C given in the exercise sheet and obtain the following:
 
 $$p(\mu_i | \vec\mu_{-i},\vec Y) \propto exp \Bigg{[} -\frac{3}{2\sigma^2} \bigg{[}  \mu_i - \frac{\mu_{i-1} + \mu_{i+1}+y_i}{3}  \bigg{]}^2 \Bigg{]} \sim N \Bigg{(} \frac{\mu_{i-1} + \mu_{i+1}+y_i}{3}, \frac{\sigma^2}{3} \Bigg{)}$$
 
 
 